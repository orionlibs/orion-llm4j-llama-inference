Orion LLM4J Llama Inference
Java library running Llama-like LLMs directly from GGUF model files

##features##
1. configurability
2. configuration validation
3. CPU-based inference with future GPU utilisation
4. any number of tokens (context length)
5. asynchronous or synchronous inference
6. system and user prompting
7. optional LLM response streaming
8. elementary inference stats

add this line to IntelliJ's compiler settings shared build VM options: --enable-preview --add-modules jdk.incubator.vector

You can download a model here: https://huggingface.co/mukel/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_0.gguf
